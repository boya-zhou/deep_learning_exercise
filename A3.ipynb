{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numerical stability\n",
    "1000000000 + 0.000001 * 1000000 - 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "      save = pickle.load(f)\n",
    "      train_dataset = save['train_dataset']\n",
    "      train_labels = save['train_labels']\n",
    "      valid_dataset = save['valid_dataset']\n",
    "      valid_labels = save['valid_labels']\n",
    "      test_dataset = save['test_dataset']\n",
    "      test_labels = save['test_labels']\n",
    "      del save  # hint to help gc free up memory\n",
    "      print('Training set', train_dataset.shape, train_labels.shape)\n",
    "      print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "      print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "      dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "      # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "      labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "      return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. \n",
    "\n",
    "Remember that L2 amounts to adding a penalty on the norm of the weights to the loss.\n",
    "\n",
    "In TensorFlow, you can compute the L2 loss for a tensor t using nn.l2_loss(t). \n",
    "\n",
    "The right amount of regularization should improve your validation / test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first define the batch size\n",
    "# I guess if want to use stochastic, must use batch\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    #first should be input\n",
    "    #placeholder should first define the type of the data\n",
    "    tf_train_data = tf.placeholder(tf.float32,shape = (batch_size,image_size * image_size))\n",
    "    tf_train_label = tf.placeholder(tf.float32,shape = (batch_size,num_labels))\n",
    "    tf_valid_data = tf.constant(valid_dataset)\n",
    "    tf_test_data = tf.constant(test_dataset)\n",
    "    lamda_reg = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #then define variable\n",
    "    weight = tf.Variable(tf.truncated_normal([image_size*image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #this part should be compute the content in each layer\n",
    "    logit = tf.matmul(tf_train_data,weight) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logit,tf_train_label)) + lamda_reg * tf.nn.l2_loss(weight)\n",
    "    \n",
    "    #define the optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # variable for display in the training proces\n",
    "    # since the accuracy function is just here, here we only need to calculate the softmax(logit)\n",
    "    train_predict = tf.nn.softmax(logit)\n",
    "    valid_predict = tf.nn.softmax(tf.matmul(tf_valid_data,weight) + biases)\n",
    "    test_predict = tf.nn.softmax(tf.matmul(tf_test_data,weight) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "      return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "              / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch at step 0: 260.066925\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 11.0%\n",
      "Minibatch at step 500: 0.885657\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch at step 1000: 1.061215\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch at step 1500: 0.841596\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch at step 2000: 0.956766\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch at step 2500: 1.044013\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch at step 3000: 1.054308\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.7%\n",
      "Test accuracy: 85.7%\n"
     ]
    }
   ],
   "source": [
    "#first define step, I guess the step here is echo\n",
    "steps = 3001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    #initialize the variable first\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    for step in range(steps):\n",
    "        # I guess offset is find where to begin with\n",
    "        offset = (step * batch_size)% (train_labels.shape[0]-batch_size)\n",
    "        batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "        batch_label = train_labels[offset:(offset+batch_size),:]\n",
    "        #feed_dict\n",
    "        feed_dict = {tf_train_data:batch_data,tf_train_label:batch_label,lamda_reg:0.08}\n",
    "        \n",
    "        #session run\n",
    "        _,l,prediction = session.run([optimizer,loss,train_predict],feed_dict = feed_dict)\n",
    "        \n",
    "        #print the info\n",
    "        if (step%500 == 0):\n",
    "            print ('Minibatch at step %d: %f'%(step,l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(prediction, batch_label))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "              valid_predict.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_predict.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pow means 10**i\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "x = np.arange(-4, -2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x138eb6d90>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERHZjpKDiIhsR8lBRES2o+QgIiLb+f8EOre6dIUNHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13ccd9750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.plot(x, regul_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg: 0.0001 : Test accuracy: 86.6%\n",
      "reg: 0.00012589254117941674 : Test accuracy: 86.8%\n",
      "reg: 0.00015848931924611142 : Test accuracy: 87.3%\n",
      "reg: 0.00019952623149688809 : Test accuracy: 87.3%\n",
      "reg: 0.00025118864315095823 : Test accuracy: 87.4%\n",
      "reg: 0.00031622776601683826 : Test accuracy: 87.4%\n",
      "reg: 0.00039810717055349773 : Test accuracy: 88.0%\n",
      "reg: 0.00050118723362727296 : Test accuracy: 88.2%\n",
      "reg: 0.00063095734448019429 : Test accuracy: 88.7%\n",
      "reg: 0.00079432823472428294 : Test accuracy: 88.7%\n",
      "reg: 0.001000000000000002 : Test accuracy: 89.0%\n",
      "reg: 0.0012589254117941701 : Test accuracy: 89.1%\n",
      "reg: 0.0015848931924611173 : Test accuracy: 89.1%\n",
      "reg: 0.001995262314968885 : Test accuracy: 89.1%\n",
      "reg: 0.0025118864315095872 : Test accuracy: 89.2%\n",
      "reg: 0.0031622776601683889 : Test accuracy: 89.1%\n",
      "reg: 0.0039810717055349856 : Test accuracy: 89.0%\n",
      "reg: 0.0050118723362727402 : Test accuracy: 89.0%\n",
      "reg: 0.0063095734448019554 : Test accuracy: 88.9%\n",
      "reg: 0.0079432823472428467 : Test accuracy: 88.7%\n"
     ]
    }
   ],
   "source": [
    "steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "x =np.arange(-4, -2, 0.1)\n",
    "result_test = []\n",
    "\n",
    "for reg in regul_val:\n",
    "    with tf.Session(graph = graph) as session:\n",
    "        #initialize the variable first\n",
    "        tf.initialize_all_variables().run()\n",
    "        #print(\"Initialized\")\n",
    "\n",
    "        for step in range(steps):\n",
    "            # I guess offset is find where to begin with\n",
    "            offset = (step * batch_size)% (train_labels.shape[0]-batch_size)\n",
    "            batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "            batch_label = train_labels[offset:(offset+batch_size),:]\n",
    "            #feed_dict\n",
    "            feed_dict = {tf_train_data:batch_data,tf_train_label:batch_label,lamda_reg:reg}\n",
    "\n",
    "            #session run\n",
    "            _,l,prediction = session.run([optimizer,loss,train_predict],feed_dict = feed_dict)\n",
    "\n",
    "            #print the info\n",
    "            #if (step%500 == 0):\n",
    "                #print ('Minibatch at step %d: %f'%(step,l))\n",
    "                #print(\"Minibatch accuracy: %.1f%%\" % accuracy(prediction, batch_label))\n",
    "                #print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                  #valid_predict.eval(), valid_labels))\n",
    "        accu_test = accuracy(test_predict.eval(), test_labels)\n",
    "        result_test.append(accu_test)\n",
    "        print(\"reg: %r : Test accuracy: %.1f%%\" %(reg,accu_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13ac88310>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEFCAYAAADqujDUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEhMKQGIiMSUEoCISEwpAYiIxNT/B8/f1WbeUdZIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a7b9550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.semilogx(regul_val,result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Add one layer netural network \n",
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    #first should be input\n",
    "    #placeholder should first define the type of the data\n",
    "    tf_train_data = tf.placeholder(tf.float32,shape = (batch_size,image_size * image_size))\n",
    "    tf_train_label = tf.placeholder(tf.float32,shape = (batch_size,num_labels))\n",
    "    tf_valid_data = tf.constant(valid_dataset)\n",
    "    tf_test_data = tf.constant(test_dataset)\n",
    "    lamda_reg = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #then define variable\n",
    "    weight1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weight2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #this part should be compute the content in each layer\n",
    "    layer1 = tf.matmul(tf_train_data,weight1) + biases1\n",
    "    logit = tf.matmul(layer1,weight2) + biases2\n",
    "    # notice that how to change the weight\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logit,tf_train_label)) + \\\n",
    "        lamda_reg * (tf.nn.l2_loss(weight1)+tf.nn.l2_loss(weight2))\n",
    "    \n",
    "    #define the optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # variable for display in the training proces\n",
    "    # since the accuracy function is just here, here we only need to calculate the softmax(logit)\n",
    "    train_predict = tf.nn.softmax(logit)\n",
    "    layer1_valid = tf.matmul(tf_valid_data,weight1) + biases1\n",
    "    valid_predict = tf.nn.softmax(tf.matmul(layer1_valid,weight2) + biases2)\n",
    "    layer2_valid = tf.matmul(tf_test_data,weight1) + biases1\n",
    "    test_predict = tf.nn.softmax(tf.matmul(layer2_valid,weight2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch at step 0: 788.691345\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 30.1%\n",
      "Minibatch at step 500: 208.850693\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch at step 1000: 125.734802\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch at step 1500: 68.923279\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch at step 2000: 42.143120\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 72.2%\n",
      "Minibatch at step 2500: 25.308464\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch at step 3000: 15.000704\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch at step 3500: 9.183514\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch at step 4000: 5.685534\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch at step 4500: 3.772083\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch at step 5000: 2.437489\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.3%\n",
      "reg: 0.00012589254117941674 : Test accuracy: 87.1%\n"
     ]
    }
   ],
   "source": [
    "steps = 5001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    #initialize the variable first\n",
    "    tf.initialize_all_variables().run()\n",
    "    #print(\"Initialized\")\n",
    "\n",
    "    for step in range(steps):\n",
    "        # I guess offset is find where to begin with\n",
    "        offset = (step * batch_size)% (train_labels.shape[0]-batch_size)\n",
    "        batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "        batch_label = train_labels[offset:(offset+batch_size),:]\n",
    "        #feed_dict\n",
    "        feed_dict = {tf_train_data:batch_data,tf_train_label:batch_label,lamda_reg:0.0025}\n",
    "\n",
    "        #session run\n",
    "        _,l,prediction = session.run([optimizer,loss,train_predict],feed_dict = feed_dict)\n",
    "\n",
    "        #print the info\n",
    "        if (step%500 == 0):\n",
    "            print ('Minibatch at step %d: %f'%(step,l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(prediction, batch_label))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "              valid_predict.eval(), valid_labels))\n",
    "    accu_test = accuracy(test_predict.eval(), test_labels)\n",
    "    result_test.append(accu_test)\n",
    "    print(\"reg: %r : Test accuracy: %.1f%%\" %(reg,accu_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg: 0.0001 : Test accuracy: 82.2%\n",
      "reg: 0.00012589254117941674 : Test accuracy: 81.6%\n",
      "reg: 0.00015848931924611142 : Test accuracy: 75.8%\n",
      "reg: 0.00019952623149688809 : Test accuracy: 78.4%\n",
      "reg: 0.00025118864315095823 : Test accuracy: 76.1%\n",
      "reg: 0.00031622776601683826 : Test accuracy: 80.3%\n",
      "reg: 0.00039810717055349773 : Test accuracy: 77.1%\n",
      "reg: 0.00050118723362727296 : Test accuracy: 78.8%\n",
      "reg: 0.00063095734448019429 : Test accuracy: 81.8%\n",
      "reg: 0.00079432823472428294 : Test accuracy: 85.6%\n",
      "reg: 0.001000000000000002 : Test accuracy: 87.1%\n",
      "reg: 0.0012589254117941701 : Test accuracy: 87.6%\n",
      "reg: 0.0015848931924611173 : Test accuracy: 87.9%\n",
      "reg: 0.001995262314968885 : Test accuracy: 88.0%\n",
      "reg: 0.0025118864315095872 : Test accuracy: 88.0%\n",
      "reg: 0.0031622776601683889 : Test accuracy: 87.9%\n",
      "reg: 0.0039810717055349856 : Test accuracy: 87.7%\n",
      "reg: 0.0050118723362727402 : Test accuracy: 87.6%\n",
      "reg: 0.0063095734448019554 : Test accuracy: 87.5%\n",
      "reg: 0.0079432823472428467 : Test accuracy: 87.3%\n"
     ]
    }
   ],
   "source": [
    "for reg in regul_val:\n",
    "    with tf.Session(graph = graph) as session:\n",
    "        #initialize the variable first\n",
    "        tf.initialize_all_variables().run()\n",
    "        #print(\"Initialized\")\n",
    "\n",
    "        for step in range(steps):\n",
    "            # I guess offset is find where to begin with\n",
    "            offset = (step * batch_size)% (train_labels.shape[0]-batch_size)\n",
    "            batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "            batch_label = train_labels[offset:(offset+batch_size),:]\n",
    "            #feed_dict\n",
    "            feed_dict = {tf_train_data:batch_data,tf_train_label:batch_label,lamda_reg:reg}\n",
    "\n",
    "            #session run\n",
    "            _,l,prediction = session.run([optimizer,loss,train_predict],feed_dict = feed_dict)\n",
    "\n",
    "            #print the info\n",
    "#             if (step%500 == 0):\n",
    "#                 print ('Minibatch at step %d: %f'%(step,l))\n",
    "#                 print(\"Minibatch accuracy: %.1f%%\" % accuracy(prediction, batch_label))\n",
    "#                 print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "#                   valid_predict.eval(), valid_labels))\n",
    "        accu_test = accuracy(test_predict.eval(), test_labels)\n",
    "        result_test.append(accu_test)\n",
    "        print(\"reg: %r : Test accuracy: %.1f%%\" %(reg,accu_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Add one layer netural network \n",
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    #first should be input\n",
    "    #placeholder should first define the type of the data\n",
    "    tf_train_data = tf.placeholder(tf.float32,shape = (batch_size,image_size * image_size))\n",
    "    tf_train_label = tf.placeholder(tf.float32,shape = (batch_size,num_labels))\n",
    "    tf_valid_data = tf.constant(valid_dataset)\n",
    "    tf_test_data = tf.constant(test_dataset)\n",
    "    lamda_reg = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #then define variable\n",
    "    weight1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weight2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #this part should be compute the content in each layer\n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_train_data,weight1) + biases1)\n",
    "    drop1 = tf.nn.dropout(layer1,0.5)\n",
    "    logit = tf.matmul(drop1,weight2) + biases2\n",
    "    # notice that how to change the weight\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logit,tf_train_label)) + \\\n",
    "        lamda_reg * (tf.nn.l2_loss(weight1)+tf.nn.l2_loss(weight2))\n",
    "    \n",
    "    #define the optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # variable for display in the training proces\n",
    "    # since the accuracy function is just here, here we only need to calculate the softmax(logit)\n",
    "    train_predict = tf.nn.softmax(logit)\n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_data,weight1) + biases1)\n",
    "    valid_predict = tf.nn.softmax(tf.matmul(layer1_valid,weight2) + biases2)\n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_data,weight1) + biases1)\n",
    "    test_predict = tf.nn.softmax(tf.matmul(layer1_test,weight2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg: 0.001 : Test accuracy: 92.9%\n",
      "reg: 0.0012589254117941675 : Test accuracy: 93.0%\n",
      "reg: 0.0015848931924611141 : Test accuracy: 92.8%\n",
      "reg: 0.0019952623149688807 : Test accuracy: 92.6%\n",
      "reg: 0.002511886431509582 : Test accuracy: 92.2%\n",
      "reg: 0.0031622776601683824 : Test accuracy: 91.7%\n",
      "reg: 0.0039810717055349778 : Test accuracy: 91.5%\n",
      "reg: 0.0050118723362727298 : Test accuracy: 91.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-000a2d5fb73d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m#session run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_predict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m#print the info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boyazhou/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boyazhou/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boyazhou/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/boyazhou/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boyazhou/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "\n",
    "for reg in regul_val:\n",
    "    with tf.Session(graph = graph) as session:\n",
    "        #initialize the variable first\n",
    "        tf.initialize_all_variables().run()\n",
    "        #print(\"Initialized\")\n",
    "\n",
    "        for step in range(steps):\n",
    "            # I guess offset is find where to begin with\n",
    "            offset = (step * batch_size)% (train_labels.shape[0]-batch_size)\n",
    "            batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "            batch_label = train_labels[offset:(offset+batch_size),:]\n",
    "            #feed_dict\n",
    "            feed_dict = {tf_train_data:batch_data,tf_train_label:batch_label,lamda_reg:reg}\n",
    "\n",
    "            #session run\n",
    "            _,l,prediction = session.run([optimizer,loss,train_predict],feed_dict = feed_dict)\n",
    "\n",
    "            #print the info\n",
    "#             if (step%500 == 0):\n",
    "#                 print ('Minibatch at step %d: %f'%(step,l))\n",
    "#                 print(\"Minibatch accuracy: %.1f%%\" % accuracy(prediction, batch_label))\n",
    "#                 print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "#                   valid_predict.eval(), valid_labels))\n",
    "        accu_test = accuracy(test_predict.eval(), test_labels)\n",
    "        result_test.append(accu_test)\n",
    "        print(\"reg: %r : Test accuracy: %.1f%%\" %(reg,accu_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add learnig rate decay and more laryers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Add one layer netural network \n",
    "## \n",
    "batch_size = 128\n",
    "hidden_nodes1 = 1024\n",
    "hidden_nodes2 = 512\n",
    "hidden_nodes3 = 256\n",
    "hidden_nodes4 = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "starter_learning_rate = 0.5\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    #first should be input\n",
    "    #placeholder should first define the type of the data\n",
    "    tf_train_data = tf.placeholder(tf.float32,shape = (batch_size,image_size * image_size))\n",
    "    tf_train_label = tf.placeholder(tf.float32,shape = (batch_size,num_labels))\n",
    "    tf_valid_data = tf.constant(valid_dataset)\n",
    "    tf_test_data = tf.constant(test_dataset)\n",
    "    lamda_reg = tf.placeholder(tf.float32)\n",
    "    # do not forget gobal step if need learning rate decay\n",
    "    global_step = tf.Variable(0,trainable=False)\n",
    "    \n",
    "    \n",
    "    #then define variable\n",
    "    weight1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_nodes1], stddev=np.sqrt(2.0 / hidden_nodes1)))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes1]))\n",
    "    weight2 = tf.Variable(tf.truncated_normal([hidden_nodes1, hidden_nodes2], stddev=np.sqrt(2.0 / hidden_nodes2)))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden_nodes2]))\n",
    "    weight3 = tf.Variable(tf.truncated_normal([hidden_nodes2, hidden_nodes3], stddev=np.sqrt(2.0 / hidden_nodes3)))\n",
    "    biases3 = tf.Variable(tf.zeros([hidden_nodes3]))\n",
    "    weight4 = tf.Variable(tf.truncated_normal([hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / hidden_nodes4)))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #this part should be compute the content in each layer\n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_train_data,weight1) + biases1)\n",
    "    drop1 = tf.nn.dropout(layer1,0.5)\n",
    "    layer2 = tf.nn.relu(tf.matmul(drop1,weight2) + biases2)\n",
    "    drop2 = tf.nn.dropout(layer2,0.5)\n",
    "    layer3 = tf.nn.relu(tf.matmul(drop2,weight3) + biases3)\n",
    "    drop3 = tf.nn.dropout(layer3,0.5)\n",
    "    logit = tf.matmul(drop3,weight4) + biases4\n",
    "    \n",
    "    \n",
    "    # notice that how to change the weight\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logit,tf_train_label)) + \\\n",
    "        lamda_reg * (tf.nn.l2_loss(weight1)+tf.nn.l2_loss(weight2)+tf.nn.l2_loss(weight3)+tf.nn.l2_loss(weight4))\n",
    "    \n",
    "    #define the optimizer\n",
    "    #if need learning rate decay, use golbal_step here\n",
    "    learning_rate_decay = tf.train.exponential_decay(starter_learning_rate,global_step,1000,0.7,staircase = True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss,global_step = global_step)\n",
    "    \n",
    "    # variable for display in the training proces\n",
    "    # since the accuracy function is just here, here we only need to calculate the softmax(logit)\n",
    "    train_predict = tf.nn.softmax(logit)\n",
    "    \n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_data,weight1) + biases1)\n",
    "    layer2_valid = tf.nn.relu(tf.matmul(layer1_valid,weight2) + biases2)\n",
    "    layer3_valid = tf.nn.relu(tf.matmul(layer2_valid,weight3) + biases3)\n",
    "    valid_predict = tf.nn.softmax(tf.matmul(layer3_valid,weight4) + biases4)\n",
    "    \n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_data,weight1) + biases1)\n",
    "    layer2_test = tf.nn.relu(tf.matmul(layer1_test,weight2) + biases2)\n",
    "    layer3_test = tf.nn.relu(tf.matmul(layer2_test,weight3) + biases3)\n",
    "    test_predict = tf.nn.softmax(tf.matmul(layer3_test,weight4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch at step 0: 8.701390\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 15.5%\n",
      "Minibatch at step 500: 2.026393\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch at step 1000: 1.258628\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch at step 1500: 0.770765\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch at step 2000: 0.623457\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch at step 2500: 0.667017\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch at step 3000: 0.689972\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch at step 3500: 0.666136\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch at step 4000: 0.650457\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch at step 4500: 0.705120\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch at step 5000: 0.774252\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.2%\n",
      "step: 5000 : Test accuracy: 92.6%\n"
     ]
    }
   ],
   "source": [
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    #initialize the variable first\n",
    "    tf.initialize_all_variables().run()\n",
    "    #print(\"Initialized\")\n",
    "\n",
    "    for step in range(steps):\n",
    "        # I guess offset is find where to begin with\n",
    "        offset = (step * batch_size)% (train_labels.shape[0]-batch_size)\n",
    "        batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "        batch_label = train_labels[offset:(offset+batch_size),:]\n",
    "        #feed_dict\n",
    "        feed_dict = {tf_train_data:batch_data,tf_train_label:batch_label,lamda_reg:0.002}\n",
    "\n",
    "        #session run\n",
    "        _,l,prediction = session.run([optimizer,loss,train_predict],feed_dict = feed_dict)\n",
    "\n",
    "        #print the info\n",
    "        if (step%500 == 0):\n",
    "            print ('Minibatch at step %d: %f'%(step,l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(prediction, batch_label))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "              valid_predict.eval(), valid_labels))\n",
    "    accu_test = accuracy(test_predict.eval(), test_labels)\n",
    "    result_test.append(accu_test)\n",
    "    print(\"step: %r : Test accuracy: %.1f%%\" %(step,accu_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Add one layer netural network \n",
    "## \n",
    "batch_size = 128\n",
    "hidden_nodes1 = 1024\n",
    "hidden_nodes2 = 512\n",
    "hidden_nodes3 = 256\n",
    "hidden_nodes4 = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "starter_learning_rate = 0.5\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    #first should be input\n",
    "    #placeholder should first define the type of the data\n",
    "    tf_train_data = tf.placeholder(tf.float32,shape = (batch_size,image_size * image_size))\n",
    "    tf_train_label = tf.placeholder(tf.float32,shape = (batch_size,num_labels))\n",
    "    tf_valid_data = tf.constant(valid_dataset)\n",
    "    tf_test_data = tf.constant(test_dataset)\n",
    "    lamda_reg = tf.placeholder(tf.float32)\n",
    "    # do not forget gobal step if need learning rate decay\n",
    "    global_step = tf.Variable(0,trainable=False)\n",
    "    \n",
    "    \n",
    "    #then define variable\n",
    "    weight1 = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_nodes1], stddev=np.sqrt(2.0 / hidden_nodes1)))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes1]))\n",
    "    weight2 = tf.Variable(tf.truncated_normal([hidden_nodes1, hidden_nodes2], stddev=np.sqrt(2.0 / hidden_nodes2)))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden_nodes2]))\n",
    "    weight3 = tf.Variable(tf.truncated_normal([hidden_nodes2, hidden_nodes3], stddev=np.sqrt(2.0 / hidden_nodes3)))\n",
    "    biases3 = tf.Variable(tf.zeros([hidden_nodes3]))\n",
    "    weight4 = tf.Variable(tf.truncated_normal([hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / hidden_nodes4)))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #this part should be compute the content in each layer\n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_train_data,weight1) + biases1)\n",
    "    drop1 = tf.nn.dropout(layer1,0.5)\n",
    "    layer2 = tf.nn.relu(tf.matmul(drop1,weight2) + biases2)\n",
    "    drop2 = tf.nn.dropout(layer2,0.5)\n",
    "    layer3 = tf.nn.relu(tf.matmul(drop2,weight3) + biases3)\n",
    "    drop3 = tf.nn.dropout(layer3,0.5)\n",
    "    logit = tf.matmul(drop3,weight4) + biases4\n",
    "    \n",
    "    \n",
    "    # notice that how to change the weight\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logit,tf_train_label)) + \\\n",
    "        lamda_reg * (tf.nn.l2_loss(weight1)+tf.nn.l2_loss(weight2)+tf.nn.l2_loss(weight3)+tf.nn.l2_loss(weight4))\n",
    "    \n",
    "    #define the optimizer\n",
    "    #if need learning rate decay, use golbal_step here\n",
    "    learning_rate_decay = tf.train.exponential_decay(starter_learning_rate,global_step,1000,0.7,staircase = True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate_decay).minimize(loss,global_step = global_step)\n",
    "    \n",
    "    # variable for display in the training proces\n",
    "    # since the accuracy function is just here, here we only need to calculate the softmax(logit)\n",
    "    train_predict = tf.nn.softmax(logit)\n",
    "    \n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_data,weight1) + biases1)\n",
    "    layer2_valid = tf.nn.relu(tf.matmul(layer1_valid,weight2) + biases2)\n",
    "    layer3_valid = tf.nn.relu(tf.matmul(layer2_valid,weight3) + biases3)\n",
    "    valid_predict = tf.nn.softmax(tf.matmul(layer3_valid,weight4) + biases4)\n",
    "    \n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_data,weight1) + biases1)\n",
    "    layer2_test = tf.nn.relu(tf.matmul(layer1_test,weight2) + biases2)\n",
    "    layer3_test = tf.nn.relu(tf.matmul(layer2_test,weight3) + biases3)\n",
    "    test_predict = tf.nn.softmax(tf.matmul(layer3_test,weight4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch at step 0: 8.509447\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 20.5%\n",
      "Minibatch at step 500: 1.915801\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch at step 1000: 1.282146\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch at step 1500: 0.752195\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch at step 2000: 0.689726\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch at step 2500: 0.679573\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch at step 3000: 0.736705\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch at step 3500: 0.683319\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch at step 4000: 0.583892\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch at step 4500: 0.567231\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch at step 5000: 0.595467\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.7%\n",
      "step: 5000 : Test accuracy: 93.9%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = graph) as session:\n",
    "    #initialize the variable first\n",
    "    tf.initialize_all_variables().run()\n",
    "    #print(\"Initialized\")\n",
    "\n",
    "    for step in range(steps):\n",
    "        # I guess offset is find where to begin with\n",
    "        offset = (step * batch_size)% (train_labels.shape[0]-batch_size)\n",
    "        batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "        batch_label = train_labels[offset:(offset+batch_size),:]\n",
    "        #feed_dict\n",
    "        feed_dict = {tf_train_data:batch_data,tf_train_label:batch_label,lamda_reg:0.002}\n",
    "\n",
    "        #session run\n",
    "        _,l,prediction = session.run([optimizer,loss,train_predict],feed_dict = feed_dict)\n",
    "\n",
    "        #print the info\n",
    "        if (step%500 == 0):\n",
    "            print ('Minibatch at step %d: %f'%(step,l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(prediction, batch_label))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "              valid_predict.eval(), valid_labels))\n",
    "    accu_test = accuracy(test_predict.eval(), test_labels)\n",
    "    result_test.append(accu_test)\n",
    "    print(\"step: %r : Test accuracy: %.1f%%\" %(step,accu_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
